{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Установка зависимостей для gguf формата моделей\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WRI9MVbfG106"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q langchain==0.0.322 pydantic openai==0.28.1 faiss-cpu==1.7.4 tiktoken==0.5.1 sentence_transformers==2.2.2 nltk==3.8.1\n",
        "!pip install -U -q deep-translator==1.11.4"
      ],
      "metadata": {
        "id": "fMHwstLiG-OI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING\"] = \"true\" # Если вы хотите отслеживать выполнение программы, установите значение «true»"
      ],
      "metadata": {
        "id": "knIt6IcYHC1b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install -q --upgrade --force-reinstall llama-cpp-python==0.2.11 --no-cache-dir"
      ],
      "metadata": {
        "id": "tywqJdHiHE8g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q numpy==1.23.1"
      ],
      "metadata": {
        "id": "WN-CKm87zqjb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import openai\n",
        "\n",
        "# Получение ключа API от пользователя и установка его как переменной окружения\n",
        "from google.colab import userdata\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "openai.api_key = openai_key"
      ],
      "metadata": {
        "id": "RfefrEkg_9tk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vFmnOH06_9tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23de5d5-abe0-410b-f6cf-50eac81144d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "qCQt9Xc2ywFk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n",
        "import requests\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "\n",
        "# функция для загрузки документа по ссылке из гугл драйв\n",
        "def load_document_text(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "8UZS4unZYxJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013126d8-995b-4380-efa0-0dda4ed2d1d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "Y1Ot3C_gzE6v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка файлов"
      ],
      "metadata": {
        "id": "YmAOCPVTQ75M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "\n",
        "documents = load_document_text(\"https://docs.google.com/document/d/1o0x1b-kSmHBzSOG5jP2tTywRi0AdygsM3FOkozO4BCI\")\n",
        "with open('documents.txt', 'w',encoding='utf8') as f:\n",
        "    f.write(documents)\n",
        "\n",
        "loader = TextLoader('/content/documents.txt')\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Используем первые 400 чанков из 867, т.к. OpenAI накладывает ограничения на бесплатном аккаунте.\n",
        "# Это повлияет на качество ответов на вопросы, которые относятся к незадействованной части базы знаний\n",
        "db = FAISS.from_documents(docs[:400], OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "CSvHpa_iwcnB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраняем папку UII_DZ_24_Lite_ru с файлами index.pkl и index.faiss\n",
        "project_path = \"/content/drive/MyDrive\"\n",
        "db.save_local(os.path.join(project_path, 'UII_DZ_24_Lite_ru'))"
      ],
      "metadata": {
        "id": "mgp91iw9elMH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраняем файл docs_new_ru.pkl (такой же как index.pkl)\n",
        "with open(os.path.join(project_path, 'docs_new_ru.pkl'), 'wb') as f:\n",
        "    pickle.dump(docs, f, protocol = None, fix_imports = True)"
      ],
      "metadata": {
        "id": "FUVpjGhCfwGA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/\"\n",
        "db = FAISS.load_local(\n",
        "    os.path.join(project_path, 'UII_DZ_24_Lite_ru'),\n",
        "    OpenAIEmbeddings(),\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "5OBuKER4M_MH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saiga2_7B_8_gguf"
      ],
      "metadata": {
        "id": "yW5-RIJe6NzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/IlyaGusev/saiga2_7b_gguf/resolve/main/model-q8_0.gguf"
      ],
      "metadata": {
        "id": "Zi_VIvrOSYAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3616fa6-023d-480b-aecf-3c6d3ffe0b64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-01 17:26:53--  https://huggingface.co/IlyaGusev/saiga2_7b_gguf/resolve/main/model-q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.61, 3.165.160.11, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/64c0641684191336fafc1202/ba18afe551fb13782bc13ad91378dc623b331f076c390c461b94bf107ad4b8cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251001%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251001T172654Z&X-Amz-Expires=3600&X-Amz-Signature=52307000ce71fa1797c74d56c885793c3589b90d92a33bd5134d603fec446c6f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q8_0.gguf%3B+filename%3D%22model-q8_0.gguf%22%3B&x-id=GetObject&Expires=1759343214&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTM0MzIxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGMwNjQxNjg0MTkxMzM2ZmFmYzEyMDIvYmExOGFmZTU1MWZiMTM3ODJiYzEzYWQ5MTM3OGRjNjIzYjMzMWYwNzZjMzkwYzQ2MWI5NGJmMTA3YWQ0YjhjZioifV19&Signature=i2VHkuWgFwVAsFA20rp12rUvFfdZkXay%7ECrB9%7E82obVqYsP9XE88qAlccWqgfaB2jJ-5MUzXHWVIrcdxlKrsAPxIK%7EsVVnIclaG6ZO1nFK2sEE%7EnH8D0Lne8GJFuE95H2KLdfR0S9z01Gi6%7ER2FFY7BMBeuwPc919hyDS%7EMLmuDZ7aj7eEo8kZ5lnMSAWsOhAz8mbGLpIG5509IwzH3oxrRKG%7E72HGGgBwHyh8YBMYZRJXgfD-efNEJZCnxyImmnAQ9B7bDPhuUVHtiXRON3P8J3WHPy5V5boKcdpP1XnmNEKuLMwteV3pbpDY8TbjwrwWrYL9bXz-dDKaxWF0IQvg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-01 17:26:54--  https://cas-bridge.xethub.hf.co/xet-bridge-us/64c0641684191336fafc1202/ba18afe551fb13782bc13ad91378dc623b331f076c390c461b94bf107ad4b8cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251001%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251001T172654Z&X-Amz-Expires=3600&X-Amz-Signature=52307000ce71fa1797c74d56c885793c3589b90d92a33bd5134d603fec446c6f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q8_0.gguf%3B+filename%3D%22model-q8_0.gguf%22%3B&x-id=GetObject&Expires=1759343214&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTM0MzIxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGMwNjQxNjg0MTkxMzM2ZmFmYzEyMDIvYmExOGFmZTU1MWZiMTM3ODJiYzEzYWQ5MTM3OGRjNjIzYjMzMWYwNzZjMzkwYzQ2MWI5NGJmMTA3YWQ0YjhjZioifV19&Signature=i2VHkuWgFwVAsFA20rp12rUvFfdZkXay%7ECrB9%7E82obVqYsP9XE88qAlccWqgfaB2jJ-5MUzXHWVIrcdxlKrsAPxIK%7EsVVnIclaG6ZO1nFK2sEE%7EnH8D0Lne8GJFuE95H2KLdfR0S9z01Gi6%7ER2FFY7BMBeuwPc919hyDS%7EMLmuDZ7aj7eEo8kZ5lnMSAWsOhAz8mbGLpIG5509IwzH3oxrRKG%7E72HGGgBwHyh8YBMYZRJXgfD-efNEJZCnxyImmnAQ9B7bDPhuUVHtiXRON3P8J3WHPy5V5boKcdpP1XnmNEKuLMwteV3pbpDY8TbjwrwWrYL9bXz-dDKaxWF0IQvg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.126, 18.238.217.63, 18.238.217.88, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7161089600 (6.7G)\n",
            "Saving to: ‘model-q8_0.gguf’\n",
            "\n",
            "model-q8_0.gguf     100%[===================>]   6.67G  76.8MB/s    in 1m 52s  \n",
            "\n",
            "2025-10-01 17:28:46 (60.8 MB/s) - ‘model-q8_0.gguf’ saved [7161089600/7161089600]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import fire\n",
        "from llama_cpp import Llama\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Ты менеджер поддержки в чате Российской компании Университет Исскуственного интеллекта. Компания продает курсы по AI.\n",
        "У компании есть большой документ со всеми материалами о продуктах компании на русском языке. Тебе задает вопрос клиент в чате, дай ему ответ на языке оригинала,\n",
        "опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально\n",
        "точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен\n",
        "знать о документе, по которому ты отвечаешь. Отвечай от первого лица без ссылок на источники на которые ты опираешься. Если ты не знаешь ответа или его нет в документе, то ответь \"Я не могу отетить на этот вопрос\".\n",
        "\"\"\"\n",
        "SYSTEM_TOKEN = 1788\n",
        "USER_TOKEN = 1404\n",
        "BOT_TOKEN = 9225\n",
        "LINEBREAK_TOKEN = 13\n",
        "\n",
        "ROLE_TOKENS = {\n",
        "    \"user\": USER_TOKEN,\n",
        "    \"bot\": BOT_TOKEN,\n",
        "    \"system\": SYSTEM_TOKEN\n",
        "}\n",
        "\n",
        "def get_message_tokens(model, role, content):\n",
        "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
        "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
        "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
        "    message_tokens.append(model.token_eos())\n",
        "    return message_tokens\n",
        "\n",
        "\n",
        "def get_system_tokens(model):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": SYSTEM_PROMPT\n",
        "    }\n",
        "    return get_message_tokens(model, **system_message)\n"
      ],
      "metadata": {
        "id": "xuZj7ZjuivJX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interact(\n",
        "    model,\n",
        "    query,\n",
        "    tokens,\n",
        "    top_k=30,\n",
        "    top_p=0.9,\n",
        "    temperature=0.2,\n",
        "    repeat_penalty=1.1\n",
        "):\n",
        "\n",
        "    answer = []\n",
        "\n",
        "    while True:\n",
        "        user_message = f\"User: {query}\"\n",
        "        message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n",
        "        role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
        "        tokens += message_tokens + role_tokens\n",
        "        generator = model.generate(\n",
        "            tokens,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            temp=temperature,\n",
        "            repeat_penalty=repeat_penalty,\n",
        "\n",
        "        )\n",
        "        for token in generator:\n",
        "            token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
        "            tokens.append(token)\n",
        "            answer.append(token_str)\n",
        "            if token == model.token_eos():\n",
        "                print('Ответ: ',''.join(answer))\n",
        "                return ''.join(answer)\n",
        "                break\n",
        "\n",
        "            print(token_str, end=\"\", flush=True)\n",
        "\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "LMdCGgLUjA2p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# инициализируем модель\n",
        "model = Llama(\n",
        "    model_path='/content/model-q8_0.gguf',\n",
        "    n_ctx=4096,\n",
        "    n_parts=1,\n",
        "    n_gpu_layers=32,\n",
        "    n_batch=512,\n",
        "    n_gqa=8,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "system_tokens = get_system_tokens(model)\n",
        "tokens = system_tokens\n",
        "model.eval(tokens)"
      ],
      "metadata": {
        "id": "eiV52Idl6UC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc94dd27-8ba3-4c28-a316-13ebbc5036e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Определяем функцию\n",
        "import re\n",
        "import time\n",
        "\n",
        "history = []\n",
        "query_time = []\n",
        "\n",
        "def callModel(question):\n",
        "    # Получаем 3 релевантных фрагмента\n",
        "    sim_docs = db.similarity_search(question, k=3)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in sim_docs])\n",
        "\n",
        "    # Формируем запрос БЕЗ лишних символов\n",
        "    user_query = f\"\"\"Ответь на вопрос клиента, используя только информацию из документа.\n",
        "\n",
        "Вопрос: {question}\n",
        "\n",
        "Информация из документа:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "    system_tokens = get_system_tokens(model)\n",
        "    tokens = system_tokens\n",
        "    model.eval(tokens)\n",
        "\n",
        "    start_time = time.time()\n",
        "    result = interact(model, query=user_query, tokens=tokens)\n",
        "    finish_time = time.time()\n",
        "\n",
        "    print(f'\\n⏱ Время обработки: {round(finish_time - start_time)} сек\\n')\n",
        "    history.append([question, result, context])\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "0Fw3HCKG6Yhs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вводим вопрос\n",
        "question = input()\n",
        "\n",
        "# получаем ответ\n",
        "result = callModel(question)\n",
        "# print (result)"
      ],
      "metadata": {
        "id": "9-ICyE_r1MTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6657b4-2a96-4ffd-964b-7ac68d94e13a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "какие тарифы предлагает Университет ИИ и чем они отличаются?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: Университет Искусственного Интеллекта предлагает несколько тарифов обучения: \"Базовый\", \"Основной\", \"Расширенный\", \"Продвинутый\", \"AI под ключ\" и \"ChatGPT\". Каждый из них предлагает уникальные возможности и глубокое погружение в мир нейронных сетей, программирования и других актуальных тем искусственного интеллекта. Независимо от уровня подготовки, студенты смогут найти подходящий тариф, соответствующий их интересам и целям.Ответ:  Ответ: Университет Искусственного Интеллекта предлагает несколько тарифов обучения: \"Базовый\", \"Основной\", \"Расширенный\", \"Продвинутый\", \"AI под ключ\" и \"ChatGPT\". Каждый из них предлагает уникальные возможности и глубокое погружение в мир нейронных сетей, программирования и других актуальных тем искусственного интеллекта. Независимо от уровня подготовки, студенты смогут найти подходящий тариф, соответствующий их интересам и целям.\n",
            "\n",
            "⏱ Время обработки: 31 сек\n",
            "\n"
          ]
        }
      ]
    }
  ]
}